{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dea4b4-6605-4c44-a9f3-e13f33dfff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1800 images belonging to 2 classes.\n",
      "Found 200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_preprocessing\n",
    "from keras_preprocessing import image\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "TRAINING_DIR = \"C:/Users\\hp\\Downloads/FIRE-SMOKE-DATASET/FIRE-SMOKE-DATASET/Train\"\n",
    "training_datagen = ImageDataGenerator(rescale=1./255,\n",
    "zoom_range=0.15,\n",
    "horizontal_flip=True,\n",
    "fill_mode='nearest')\n",
    "VALIDATION_DIR = \"C:/Users\\hp\\Downloads/FIRE-SMOKE-DATASET/FIRE-SMOKE-DATASET/Test\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = training_datagen.flow_from_directory(\n",
    "TRAINING_DIR,\n",
    "target_size=(224,224),\n",
    "shuffle = True,\n",
    "class_mode='categorical',\n",
    "batch_size = 128)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "VALIDATION_DIR,\n",
    "target_size=(224,224),\n",
    "class_mode='categorical',\n",
    "shuffle = True,\n",
    "batch_size= 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44f3bb7-e982-4ed5-9459-ae491d03db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 53s 4s/step - loss: 7.8758 - acc: 0.8020 - val_loss: 0.1225 - val_acc: 0.9439\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 59s 4s/step - loss: 0.0823 - acc: 0.9683 - val_loss: 0.2242 - val_acc: 0.9286\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.3626 - acc: 0.8959 - val_loss: 0.1375 - val_acc: 0.9592\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.2689 - acc: 0.9103 - val_loss: 0.1011 - val_acc: 0.9643\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.1757 - acc: 0.9384 - val_loss: 0.1329 - val_acc: 0.9592\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.1494 - acc: 0.9569 - val_loss: 1.1256 - val_acc: 0.6684\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 53s 4s/step - loss: 0.1272 - acc: 0.9575 - val_loss: 0.1818 - val_acc: 0.9337\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.1761 - acc: 0.9402 - val_loss: 0.1160 - val_acc: 0.9643\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.0640 - acc: 0.9731 - val_loss: 0.2260 - val_acc: 0.9337\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.1305 - acc: 0.9581 - val_loss: 0.1402 - val_acc: 0.9541\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 57s 4s/step - loss: 0.1147 - acc: 0.9727 - val_loss: 0.2219 - val_acc: 0.9541\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 56s 4s/step - loss: 0.1242 - acc: 0.9635 - val_loss: 0.1580 - val_acc: 0.9541\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.0717 - acc: 0.9743 - val_loss: 0.1092 - val_acc: 0.9643\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 55s 4s/step - loss: 0.0506 - acc: 0.9833 - val_loss: 0.9520 - val_acc: 0.8673\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 55s 4s/step - loss: 0.1018 - acc: 0.9767 - val_loss: 0.1269 - val_acc: 0.9643\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 54s 4s/step - loss: 0.0574 - acc: 0.9815 - val_loss: 0.1382 - val_acc: 0.9541\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 55s 4s/step - loss: 0.2502 - acc: 0.9593 - val_loss: 0.1192 - val_acc: 0.9643\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 55s 4s/step - loss: 0.0290 - acc: 0.9886 - val_loss: 0.1288 - val_acc: 0.9694\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 56s 4s/step - loss: 0.0392 - acc: 0.9844 - val_loss: 0.1794 - val_acc: 0.9643\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 56s 4s/step - loss: 0.0214 - acc: 0.9916 - val_loss: 0.2071 - val_acc: 0.9541\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Dropout\n",
    "input_tensor = Input(shape=(224, 224, 3))\n",
    "base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "for layer in base_model.layers:\n",
    "  layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "train_generator,\n",
    "steps_per_epoch = 14,\n",
    "epochs = 20,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2450dd-1d25-4162-8891-38c9511dc35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.2910 - acc: 0.8780 - val_loss: 0.1586 - val_acc: 0.9643\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.2217 - acc: 0.9103 - val_loss: 0.1390 - val_acc: 0.9592\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.1515 - acc: 0.9420 - val_loss: 0.1306 - val_acc: 0.9643\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.1115 - acc: 0.9581 - val_loss: 0.1256 - val_acc: 0.9694\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.0882 - acc: 0.9677 - val_loss: 0.1225 - val_acc: 0.9694\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 84s 6s/step - loss: 0.0836 - acc: 0.9677 - val_loss: 0.1209 - val_acc: 0.9694\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.0704 - acc: 0.9791 - val_loss: 0.1198 - val_acc: 0.9643\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 82s 6s/step - loss: 0.0618 - acc: 0.9803 - val_loss: 0.1189 - val_acc: 0.9643\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 83s 6s/step - loss: 0.0614 - acc: 0.9827 - val_loss: 0.0998 - val_acc: 0.9694\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 90s 6s/step - loss: 0.0515 - acc: 0.9833 - val_loss: 0.1030 - val_acc: 0.9694\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:249]:\n",
    "  layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "  layer.trainable = True\n",
    "#Recompile the model for these modifications to take effect\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(\n",
    "train_generator,\n",
    "steps_per_epoch = 14,\n",
    "epochs = 10,\n",
    "validation_data = validation_generator,\n",
    "validation_steps = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f30c45-e0d8-49c2-98de-dce58cd3690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d511601-bcee-405a-b62f-b8e14b9942e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.5.3.56-cp38-cp38-win_amd64.whl (34.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\new\\anaconda3\\lib\\site-packages (from opencv-python) (1.18.5)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.3.56\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17a6baae-8638-4614-b621-3f594250f1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing import image\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "video = cv2.VideoCapture(0)\n",
    "while True:\n",
    "        _, frame = video.read()\n",
    "        im = Image.fromarray(frame, 'RGB')\n",
    "        im = im.resize((224,224))\n",
    "        img_array = image.img_to_array(im)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255\n",
    "        probabilities = model.predict(img_array)[0]\n",
    "        prediction = np.argmax(probabilities)\n",
    "        if prediction == 0:\n",
    "                #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "                #print(probabilities[prediction])\n",
    "                print(1)\n",
    "        cv2.imshow(\"Capturing\", frame)\n",
    "        \n",
    "        # provide IoT quit identifier\n",
    "        key=cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "                break\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd18498e-e279-4978-b2bc-5fe1cb591fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
